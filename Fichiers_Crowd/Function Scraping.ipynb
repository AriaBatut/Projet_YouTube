{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cdc27dd",
   "metadata": {},
   "source": [
    "# Projet E4 : Profils de Youtubeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89626364",
   "metadata": {},
   "source": [
    "Aria BATUT, Victor CHAU, Alexandre SAIPHOU, Pauline SOLERE,\n",
    "Baptiste TOUATI, Thomas SAVIANA et Sébastien WARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2c0f7",
   "metadata": {},
   "source": [
    "## Partie Crowdfunding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4492ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import Request, urlopen \n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from urlextract import URLExtract\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os, json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feeee18",
   "metadata": {},
   "source": [
    "La fonction suivante prend en paramètre un URL de vidéo Youtube et retourne les liens de crowdfunding présent dans la description de la vidéo. Il est aussi possible de retourner seulement quelques sites de crowdfunding choisis en changeant les valeurs dans \"list_crowdfunding\" et \"list_crowdfunding_avec_maj\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c414f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_get_link_donation(url): #la fonction prend en paramètre l'url d'une vidéo youtube\n",
    "    req = Request(url , headers={'User-Agent' : 'Mozilla'})\n",
    "    webpage = urlopen(req).read()\n",
    "    page_soup = bs(webpage, \"html.parser\") #utilisation de BeautifulSoup pour récupérer la page web\n",
    "\n",
    "    user_agent_list = [\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5)  AppleWebKit/537.36   (KHTML, like Gecko) Chrome/83.0.4103.97  Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)        AppleWebKit/537.36   (KHTML, like Gecko) Chrome/83.0.4103.97  Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)        AppleWebKit/537.36   (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)  AppleWebKit/537.36   (KHTML, like Gecko) Chrome/39.0.2171.95  Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5)  AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1       Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0)     Gecko/20100101 Firefox/77.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "    ]\n",
    "    \n",
    "    #dans la code source de la page web on cherche toutes les balises HTML \"script\"\n",
    "    one_a_tag = page_soup.findAll('script')\n",
    "    one_a_tagS = str(one_a_tag) #transforme toute ce qu'il y a sous la balise script en string\n",
    "    one_a_tag1 = one_a_tagS.split(\"\\\"description\\\":\") #on isole ce qui se trouve avant et après description\n",
    "    one_a_tag2 = one_a_tag1[1].split(\"}\") #on prend tout ce qui est après description (première case) jusqu'à '}'\n",
    "    \n",
    "    resHttp = [i for i in range(len(one_a_tag2[0])) if one_a_tag2[0].startswith(\"http\", i)] #dans la partie entre description et '}' on prend tout ce qui commence par http\n",
    "\n",
    "    resN = [i for i in range(len(one_a_tag2[0])) if one_a_tag2[0].startswith(\" \", i) or one_a_tag2[0].startswith(\"\\\\n\", i)] #dans la partie entre description et '}' on prend tout ce qui commence par un espace ou un saut de ligne\n",
    "\n",
    "    urls = []\n",
    "    for x in resHttp:\n",
    "        b = 1 #pour ne pas reprendre plusieurs fois le même lien on met une condition en plus sur b\n",
    "        for y in resN:\n",
    "            if b == 1 and x < y: #si le http est avant l'espace ou le saut de ligne\n",
    "                s = slice(x,y) #alors on prend tout ce qu'il y a entre le http et l'espace ou le saut de ligne\n",
    "                urls.append(one_a_tag2[0][s]) #on stocke le résultat précédent dans urls\n",
    "                b = 0\n",
    "                \n",
    "    urls_final = []\n",
    "    \n",
    "    #La première liste de sites de crowdfunding est pour vérifier si dans les liens que nous avons trouvés \n",
    "    #précédemment certain sont des sites de crowdfunding (il n'y a donc pas de majuscule dans des URL)\n",
    "    list_crowdfunding = ['tipeee','patreon','clipeee','utip','leetchi','ulule','kickstarter','kisskissbankbank','wiseed','indiegogo','lendopolis','wesharebonds','mymajorcompany','gofundme']\n",
    "    \n",
    "    #La deuxième liste est pour vérifier les sites avec 'bit' (qui sont des liens réduits), on regarde donc dans\n",
    "    #le titre de la page source si cela correspond à un site de crowdfunding il faut donc avec toutes les écritures possibles (avec ou sans majuscules)\n",
    "    list_crowdfunding_avec_maj = ['tipeee','Tipeee','patreon','Patreon','clipeee','Clipeee','utip','uTip','leetchi','Leetchi','ulule','Ulule','kickstarter','Kickstarter','kisskissbankbank','KissKissBankBank','wiseed','Wiseed','indiegogo','Indiegogo','lendopolis','Lendopolis','wesharebonds','WeShareBonds','mymajorcompany','MyMajorCompany','gofundme','GoFundMe']\n",
    "\n",
    "    for i in range(len(urls)):\n",
    "        for j in range(len(list_crowdfunding)):\n",
    "            if list_crowdfunding[j] in urls[i]: #si un des sites de crowdfunding est présent dans l'URL on le stocke dans urls_final\n",
    "                urls_final.append(urls[i]) \n",
    "\n",
    "        if 'bit' in urls[i]: #ouvre la page source des lien avec 'bit' dedans et on regarde si le titre de la page correspond à un site de crowdfunding\n",
    "            urli = urls[i]\n",
    "            reqi = Request(urli , headers={'User-Agent': random.choice(user_agent_list)}) #varier les user-agent permet d'éviter d'être bloqué par le site web\n",
    "            webpage = urlopen(reqi).read()\n",
    "            page_soup = bs(webpage, \"html.parser\")\n",
    "            one_a_tag_for = page_soup.findAll('title')\n",
    "            for k in range(len(list_crowdfunding_avec_maj)):\n",
    "                if list_crowdfunding_avec_maj[k] in one_a_tag_for:\n",
    "                    urls_final.append(urls[i]) \n",
    "                \n",
    "    #on met ensuite le résultat dans un data frame et on l'exporte en csv\n",
    "    df_donation = pd.DataFrame({'lien crowdfundng': urls_final}) \n",
    "    df_donation.to_csv('function_get_donation.csv')\n",
    "            \n",
    "    return urls_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b470c",
   "metadata": {},
   "source": [
    "Exemple avec un url quelconque pour montrer la sytaxe de div class=\"p-value system-txt\" (pour tipeee) et span class = \"blue-text semi-bold mr-1\" (pour utip). Comme on peut le voir sur les résultats suivant pour Tipeee dans la classe p-value system-txt il y a deux informations : les dons en premier et le nombre de Tipers en second. Pour uTip on a simplement l'information des abonnées dans la classe blue-text semi-bold mr-1. On se sert donc de ces informations trouvées sur dans la page source pour créer l'algorithme de la fonction suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf1d043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tw/7wfxm5gj5yncxf8mnytw_vqw0000gn/T/ipykernel_30840/88174564.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path = path, options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipeee\n",
      "[<div class=\"p-value system-txt\">491 €</div>, <div class=\"p-value system-txt\">194</div>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tw/7wfxm5gj5yncxf8mnytw_vqw0000gn/T/ipykernel_30840/88174564.py:17: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path = path, options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uTip\n",
      "[<span class=\"blue-text semi-bold mr-1\" data-v-f2780b1a=\"\">525</span>]\n"
     ]
    }
   ],
   "source": [
    "#TIPEEE\n",
    "path = '/usr/local/bin/chromedriver'\n",
    "options = Options() \n",
    "options.headless = True \n",
    "driver = webdriver.Chrome(executable_path = path, options=options)\n",
    "driver.get('https://fr.tipeee.com/max-bird')\n",
    "time.sleep(3)\n",
    "soup_level1=bs(driver.page_source, 'lxml')\n",
    "mydivs_class_tipeee = soup_level1.findAll(\"div\", {\"class\": \"p-value system-txt\"})\n",
    "print(\"Tipeee\")\n",
    "print(mydivs_class_tipeee)\n",
    "\n",
    "#uTip\n",
    "path = '/usr/local/bin/chromedriver'\n",
    "options = Options() \n",
    "options.headless = True \n",
    "driver = webdriver.Chrome(executable_path = path, options=options)\n",
    "driver.get(\"https://utip.io/polodebile\")\n",
    "time.sleep(3)\n",
    "soup_level1=bs(driver.page_source, 'lxml')\n",
    "mydivs_class_utip = soup_level1.findAll(\"span\", {\"class\": \"blue-text semi-bold mr-1\"})\n",
    "print(\"uTip\")\n",
    "print(mydivs_class_utip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a26a94",
   "metadata": {},
   "source": [
    "La fonction suivante prend en argument un url de site de crowdfunding, le résultat est l'url donné, le don affiché sur le site, le nombre d'abonnés affichés sur le site et le temps qui correspond à la date/heure où l'information a été prise sur le site. Cette fois-ci on utilise un web driver car les informations sont actualisées en temps réelle donc une autre technologie (Java) est utilisée par les sites de crowdfunding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cce5133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_get_sum_donation(url): \n",
    "    path = '/usr/local/bin/chromedriver' #il faut télécharger le webdriver selon la version chrome que l'on possède et donner le chemin où se site le webdriver téléchargé\n",
    "    options = Options() #les options mises permettent d'éviter d'être bloqué par le navigateur\n",
    "    options.headless = True \n",
    "    options.add_argument('--no-sandbox') \n",
    "    driver = webdriver.Chrome(executable_path = path, options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    found_don = []\n",
    "    found_abo = []\n",
    "    \n",
    "    soup_level1=bs(driver.page_source, 'lxml')\n",
    "    \n",
    "    if 'tipeee' in url: #cas si l'url est un lien TIPEEE\n",
    "        mydivs_class = soup_level1.findAll(\"div\", {\"class\": \"p-value system-txt\"}) #on stocke toutes les valeurs de la classe voulue\n",
    "        for i in range(len(mydivs_class)):\n",
    "            str_mydivs_class = str(mydivs_class[i])\n",
    "            if \"€\" in str_mydivs_class: #si il y a un € dans la \"case\" on stocke sa valeur dans tab_don car c'est une valeur monétaire\n",
    "                str_tab_don = str(mydivs_class[i])\n",
    "                found_don=re.search('>(.+?)<', str_tab_don).group(1)\n",
    "            else: #sinon on stocke la valeur dans les abonnés\n",
    "                str_tab_tiper = str(mydivs_class[i])\n",
    "                found_abo=re.search('>(.+?)<', str_tab_tiper).group(1)\n",
    "            \n",
    "    if 'utip' in url: #cas si l'url est un lien uTip\n",
    "        mydivs_class = soup_level1.findAll(\"span\", {\"class\": \"blue-text semi-bold mr-1\"}) #on stocke toutes les valeurs de la classe voulue\n",
    "        for i in range(len(mydivs_class)):\n",
    "            str_tab_tiper = str(mydivs_class[i])\n",
    "            found_abo=re.search('>(.+?)<', str_tab_tiper).group(1)\n",
    "            \n",
    "    time_now = str(datetime.now())\n",
    "    driver.quit()\n",
    "    return url, found_don, found_abo, time_now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c700e1",
   "metadata": {},
   "source": [
    "On teste ensuite les deux fonctions sur un exemple concret : une vidéo de Max Bird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59cd6bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tw/7wfxm5gj5yncxf8mnytw_vqw0000gn/T/ipykernel_30840/3512483790.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path = path, options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................\n",
      "Result 1 : function_get_link_donation\n",
      "['https://fr.tipeee.com/max-bird', 'https://www.clipeee.com/creator/max-bird)']\n",
      " \n",
      "Result 2 : function_get_sum_donation\n",
      "                                         url    Don  Abo  \\\n",
      "0             https://fr.tipeee.com/max-bird  491 €  194   \n",
      "1  https://www.clipeee.com/creator/max-bird)     []   []   \n",
      "\n",
      "                        Temps  \n",
      "0  2022-04-18 20:16:00.261773  \n",
      "1  2022-04-18 20:16:06.671843  \n",
      "..................................\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.youtube.com/watch?v=dayHLiePIXo'\n",
    "urls_final = function_get_link_donation(url)\n",
    "\n",
    "url_final = []\n",
    "found_don_final = []\n",
    "found_abo_final = []\n",
    "time_now_final = []\n",
    "\n",
    "for i in range(len(urls_final)):\n",
    "    url_f, found_don, found_abo, time_now = function_get_sum_donation(urls_final[i])\n",
    "    url_final.append(url_f)\n",
    "    found_don_final.append(str(found_don))\n",
    "    found_abo_final.append(found_abo)\n",
    "    time_now_final.append(time_now)\n",
    "    \n",
    "df_url_don_abo_time=pd.DataFrame({'url': url_final,'Don': found_don_final, 'Abo': found_abo_final, 'Temps':time_now_final})\n",
    "print('..................................')\n",
    "print('Result 1 : function_get_link_donation')\n",
    "print(urls_final)\n",
    "print(\" \")\n",
    "print('Result 2 : function_get_sum_donation')\n",
    "print(df_url_don_abo_time)\n",
    "print('..................................')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16553ed6",
   "metadata": {},
   "source": [
    "### Fichier excel WIZDEO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88be49",
   "metadata": {},
   "source": [
    "L'objectif maintenant est de faire tourner nos fonctions créées sur des données réelles. Nous allons dans un premier temps utiliser le fichier excel 2021_10_08_FR_fr_Channels_LISIS_STUDENTS.xlsx et notamment la colonne summary. Celle-ci décrit la chaîne et nous avons cherché si des liens de crowdfunding étaient présents dedans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f517eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_chaine = pd.read_excel('2021_10_08_FR_fr_Channels_LISIS_STUDENTS.xlsx') #on ouvre le fichier excel\n",
    "df = pd.DataFrame(fichier_chaine) #on le transforme en data frame\n",
    "df_chaine_summary = pd.DataFrame({'display_name': fichier_chaine.display_name, 'Sommaire': fichier_chaine.summary}) #on garde seulement ce qu'on a besoin le display name et le summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1755b5",
   "metadata": {},
   "source": [
    "On cherche donc dans la colonne summary les liens tipeee. L'algorithme est sur le même principe que celui fait pour la description youtube. Le principal changement est l'utilisation de URLExtract() qui permet de sortir tous les url présent dans une chaine de caractère. Nous pouvons aussi sortir tous les liens de crowdfunding en changeant la variable 'list_crowdfunding' mais nous avons préféré dans un premier temps nous concentrer sur tipeee. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47fa9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "lien_crowdfunding = []\n",
    "chaine2_crowdfunding = []\n",
    "lien = []\n",
    "crowdfunding = []\n",
    "chaine = []\n",
    "\n",
    "#list_crowdfunding = ['tipeee','tipeeestream','patreon','clipeee','utip','leetchi','ulule','kickstarter','kisskissbankbank','wiseed','indiegogo','lendopolis','wesharebonds','mymajorcompany','gofundme','paypal']\n",
    "list_crowdfunding = ['tipeee']\n",
    "                     \n",
    "for i in range(len(df_chaine_summary)): #on extrait tous les URL présent dans les descriptions de chaîne\n",
    "    extractor = URLExtract()\n",
    "    urls = extractor.find_urls(str(df_chaine_summary.Sommaire[i]))\n",
    "    lien_crowdfunding.append(urls)\n",
    "    chaine2_crowdfunding.append(df_chaine_summary.display_name[i]) #on stocke les liens de crowdfunding d'un côté et les noms de chaîne\n",
    "df_chaine_lien = pd.DataFrame({'display_name': chaine2_crowdfunding,'Lien_crowdfunding': lien_crowdfunding})\n",
    "df_chaine_lien.to_csv('Chaine_Lien.csv')\n",
    "\n",
    "for i in range(len(df_chaine_lien)): #on vérifie pour chaque lien trouvé si c'est un lien de crowdfunding Tipeee \n",
    "    for j in range(len(list_crowdfunding)): \n",
    "        if list_crowdfunding[j] in str(df_chaine_lien.Lien_crowdfunding[i]): #pour chaque ligne on regarde s'il y a un site de crowdfunding dedans\n",
    "            for k in range(len(df_chaine_lien.Lien_crowdfunding[i])): #il y a parfois plusieurs liens pour une chaîne on doit donc prendre dans ces liens seulement ceux de crowdfunding  \n",
    "                if list_crowdfunding[j] in str(df_chaine_lien.Lien_crowdfunding[i][k]):\n",
    "                    found = re.findall(list_crowdfunding[j], str(df_chaine_lien.Lien_crowdfunding[i])) #permet de trouver le nom du site de crowdfunding\n",
    "                    crowdfunding.append(found)\n",
    "                    lien.append(df_chaine_lien.Lien_crowdfunding[i][k])\n",
    "                    chaine.append(df_chaine_lien.display_name[i])\n",
    "            \n",
    "df_chaine_crowd_lien = pd.DataFrame({'display_name': chaine,'Crowdfunding': crowdfunding, 'Lien': lien})\n",
    "df_chaine_crowd_lien.to_csv('Chaine_Lien_crowdfunding_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff083cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>display_name</th>\n",
       "      <th>Crowdfunding</th>\n",
       "      <th>Lien</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ScienceEtonnante</td>\n",
       "      <td>[tipeee]</td>\n",
       "      <td>https://www.tipeee.com/science-etonnante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kriss Papillon</td>\n",
       "      <td>[tipeee]</td>\n",
       "      <td>https://www.tipeee.com/kriss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MaxEstLa</td>\n",
       "      <td>[tipeee]</td>\n",
       "      <td>https://www.tipeeestream.com/maxestla/donation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Caljbeut</td>\n",
       "      <td>[tipeee]</td>\n",
       "      <td>https://www.tipeee.com/caljbeut-cartoons-trashs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Laupok</td>\n",
       "      <td>[tipeee]</td>\n",
       "      <td>tipeee.com/laupok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       display_name Crowdfunding  \\\n",
       "0  ScienceEtonnante     [tipeee]   \n",
       "1    Kriss Papillon     [tipeee]   \n",
       "2          MaxEstLa     [tipeee]   \n",
       "3          Caljbeut     [tipeee]   \n",
       "4            Laupok     [tipeee]   \n",
       "\n",
       "                                              Lien  \n",
       "0         https://www.tipeee.com/science-etonnante  \n",
       "1                     https://www.tipeee.com/kriss  \n",
       "2   https://www.tipeeestream.com/maxestla/donation  \n",
       "3  https://www.tipeee.com/caljbeut-cartoons-trashs  \n",
       "4                                tipeee.com/laupok  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chaine_crowd_lien.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f47fa",
   "metadata": {},
   "source": [
    "Une fois qu'on a trouvé tous les sites de crowdfunding et plus particulièrement de Tipeee l'objectif est donc de trouver les dons et les adonnées sur le site, nous allons donc utiliser l'algorithme créé au début."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tw/7wfxm5gj5yncxf8mnytw_vqw0000gn/T/ipykernel_30840/3512483790.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path = path, options=options)\n"
     ]
    }
   ],
   "source": [
    "don_tipeee = []\n",
    "don_tiper = []\n",
    "lien_tipeee = []\n",
    "time_final = []\n",
    "chaine_tipeee = []\n",
    "crowd_tipeee = []\n",
    "\n",
    "for i in range(len(df_chaine_crowd_lien.Lien)): # certains liens ne sont pas au format correct (pas de http ou https) on doit donc les rajouter ce qui manque\n",
    "    if 'https://' not in df_chaine_crowd_lien.Lien[i]:\n",
    "        if 'http://' not in df_chaine_crowd_lien.Lien[i]:\n",
    "            df_chaine_crowd_lien.Lien[i] = 'https://www.' + df_chaine_crowd_lien.Lien[i]\n",
    "\n",
    "    url_f, found_don_f, found_tiper_f, time_f = function_get_sum_donation(df_chaine_crowd_lien.Lien[i]) #on utilise la fonction function_get_sum_donation pour avoir le don et les abonnés\n",
    "    don_tipeee.append(found_don_f) #on stocke ensuite les résultats dans des listes\n",
    "    don_tiper.append(found_tiper_f)\n",
    "    time_final.append(time_f)\n",
    "    lien_tipeee.append(url_f)\n",
    "    chaine_tipeee.append(df_chaine_crowd_lien.display_name[i])\n",
    "    crowd_tipeee.append(df_chaine_crowd_lien.Crowdfunding[i])\n",
    "    \n",
    "df_chaine_crowd_lien_don=pd.DataFrame({'display_name': chaine_tipeee,'Crowdfunding': crowd_tipeee, 'Lien': lien_tipeee, 'Don': don_tipeee, 'Nb Tipers': don_tiper, 'Temps':time_final})\n",
    "print(df_chaine_crowd_lien_don)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chaine_crowd_lien_don.to_csv('Chaine_Lien_Dons_crowdfunding_tipers_time_summary_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f34adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chaine_crowd_lien_don.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4ce84",
   "metadata": {},
   "source": [
    "Ensuite on rassemble le fichier excel d'origine avec le fichier qu'on vient de créer pour avoir toutes les colonnes présentes et pouvoir mieux analyser les données. On utilise donc merge pour rassembler les excels selon la colonne \"disply_name\" présent dans les deux fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'chaine_crowd_lien_don_f.xlsx'\n",
    "file_name2 = '2021_10_08_FR_fr_Channels_LISIS_STUDENTS.xlsx'\n",
    "df_chaine_crowd_lien_don.to_excel(file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d840cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = df_chaine_crowd_lien_don.merge(df, on = \"display_name\",  how = \"left\") \n",
    "f3.to_excel(\"Results_f.xlsx\", index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3338b60",
   "metadata": {},
   "source": [
    "### FICHIER JSON crowd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a26f3",
   "metadata": {},
   "source": [
    "Dans cette partie nous allons utiliser les fichiers JSON donnés par WIZDEO et qui possèdent des liens de crowdfundig dedans. Ces liens sont issus d'un certain nombre de vidéo youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lister tous les fichiers json présent dans le dossier donné par WIZDEO\n",
    "path_to_json = '/Users/pauline/Downloads/crowd'\n",
    "json_files = [json for json in os.listdir(path_to_json)]\n",
    "print(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c90c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = []\n",
    "part1 = []\n",
    "nom_crowd = []\n",
    "lien_crowd = []\n",
    "data_json_part1 = []\n",
    "id_yt = []\n",
    "\n",
    "# récupération des wa_id pour l'identification des youtubeurs\n",
    "for i in range(len(json_files)):\n",
    "    data_json_files = str(json_files[i])\n",
    "    one_a_tag_ = data_json_files.split(\"_\") #on utilise split pour diviser la chaine de caractère et récupérer seulement le chiffre entre 'crowd_' et '.json'\n",
    "    data_json_part1.append(one_a_tag_[1])\n",
    "    data_json_part1_str = str(data_json_part1[i])\n",
    "    one_a_tag_bis = data_json_part1_str.split(\".\")\n",
    "    id_yt.append(one_a_tag_bis[0])\n",
    "    #pour chaque fichier JSON on met tout ce qu'il y a dedans dans un tableau pour récupérer les liens de crowdfunding de chaque fichier\n",
    "    with open('/Users/pauline/Downloads/crowd/'+json_files[i]) as mon_fichier:\n",
    "        data = json.load(mon_fichier)\n",
    "        data_f.append(data)\n",
    "#on réalise une autre boucle for pour diviser la partie lien dans le fichier json, car celle-ci est composée du nom du site de crowdfunding et du lien\n",
    "for j in range(len(data_f)):\n",
    "    data_f_j = str(data_f[j])\n",
    "    one_a_tag1 = data_f_j.split(\"'\") \n",
    "    part1.append(one_a_tag1)\n",
    "    nom_crowd.append(part1[j][1])\n",
    "    lien_crowd.append(part1[j][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_yt #résultat de la récupération des wa_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fee14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(json_files)) #nombre de fichier json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tableau avec wa_id, les noms de crowd et les liens\n",
    "lien_tipeee = []\n",
    "crowd_tipeee = []\n",
    "id_yt_f = []\n",
    "\n",
    "for i in range(len(lien_crowd)):\n",
    "    lien_tipeee.append(lien_crowd[i])\n",
    "    crowd_tipeee.append(nom_crowd[i])\n",
    "    id_yt_f.append(id_yt[i])\n",
    "    \n",
    "df_crowd_lien_id=pd.DataFrame({'wa_id': id_yt_f, 'Crowdfunding': crowd_tipeee, 'Lien': lien_tipeee})\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crowd_lien_id.Crowdfunding.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755129b",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir d'après le résultat précédent les sites les plus utilisés sont uTip et Tipeee, nous allons donc nous concentrer sur eux pour la suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2de78a",
   "metadata": {},
   "source": [
    "### Seulement TIPEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on isole les lien tipeee\n",
    "df_crowd_lien_id_tipeee = df_crowd_lien_id[list(df_crowd_lien_id.Crowdfunding == 'tipeee')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crowd_lien_id_tipeee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ee8fb",
   "metadata": {},
   "source": [
    "On utilise l'algorithme function_get_sum_donation pour avoir le don et le nombre d'abonnés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf611f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "lien_crowd = list(df_crowd_lien_id_tipeee.Lien)\n",
    "nom_crowd = list(df_crowd_lien_id_tipeee.Crowdfunding)\n",
    "don_tipeee = []\n",
    "don_tiper = []\n",
    "lien_tipeee = []\n",
    "time_final = []\n",
    "crowd_tipeee = []\n",
    "id_yt_f = []\n",
    "\n",
    "for i in range(len(lien_crowd)):\n",
    "    \n",
    "    url_f, found_don_f, found_tiper_f, time_f = function_get_sum_donation(lien_crowd[i])\n",
    "    don_tipeee.append(found_don_f) \n",
    "    don_tiper.append(found_tiper_f)\n",
    "    time_final.append(time_f)\n",
    "    lien_tipeee.append(url_f)\n",
    "    crowd_tipeee.append(nom_crowd[i])\n",
    "    id_yt_f.append(id_yt[i])\n",
    "    print(i)\n",
    "    print(lien_crowd[i])\n",
    "    \n",
    "df_crowd_lien_don_tipers_tipeee=pd.DataFrame({'wa_id': id_yt_f, 'Crowdfunding': crowd_tipeee, 'Lien': lien_tipeee, 'Don': don_tipeee, 'Nb Tipers': don_tiper, 'Temps':time_final})\n",
    "df_crowd_lien_don_tipers_tipeee.to_csv('Lien_Dons_crowdfunding_tipers_time_JSON_tipeee_f.csv')\n",
    "df_crowd_lien_don_tipers_tipeee.to_excel('Lien_Dons_crowdfunding_tipers_time_JSON_tipeee_f.xlsx') \n",
    "print('end')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf9f737",
   "metadata": {},
   "source": [
    "### Seulement UTIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba449019",
   "metadata": {},
   "source": [
    "On réalise les mêmes étapes que précédemment mais pour uTip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc42cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crowd_lien_id_utip = df_crowd_lien_id[list(df_crowd_lien_id.Crowdfunding == 'utip')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210280ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crowd_lien_id_utip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a51163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "lien_crowd = list(df_crowd_lien_id_utip.Lien)\n",
    "nom_crowd = list(df_crowd_lien_id_utip.Crowdfunding)\n",
    "don_utip = []\n",
    "don_abo = []\n",
    "lien_utip = []\n",
    "time_final = []\n",
    "crowd_utip = []\n",
    "id_yt_f = []\n",
    "\n",
    "for i in range(len(lien_crowd)):\n",
    "    url_f, found_don_f, found_tiper_f, time_f = function_get_sum_donation(lien_crowd[i])\n",
    "    don_utip.append(found_don_f) \n",
    "    don_abo.append(found_tiper_f)\n",
    "    time_final.append(time_f)\n",
    "    lien_utip.append(url_f)\n",
    "    crowd_utip.append(nom_crowd[i])\n",
    "    id_yt_f.append(id_yt[i])\n",
    "    \n",
    "df_crowd_lien_don_tipers_utip=pd.DataFrame({'wa_id': id_yt_f, 'Crowdfunding': crowd_utip, 'Lien': lien_utip, 'Don': don_utip, 'Nb Abo': don_abo, 'Temps':time_final})\n",
    "df_crowd_lien_don_tipers_utip.to_csv('Lien_Dons_crowdfunding_tipers_time_JSON_utip_f.csv')\n",
    "df_crowd_lien_don_tipers_utip.to_excel('Lien_Dons_crowdfunding_tipers_time_JSON_utip_f.xlsx') \n",
    "print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a55ddf",
   "metadata": {},
   "source": [
    "Ensuite on rassemble le fichier excel d'origine avec les fichiers qu'on vient de créer pour avoir toutes les colonnes présentes et pouvoir mieux analyser les données. On utilise donc merge pour rassembler les excels selon la colonne \"wa_id\" présent dans les deux fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_chaine = pd.read_excel('2021_10_08_FR_fr_Channels_LISIS_STUDENTS.xlsx')\n",
    "df = pd.DataFrame(fichier_chaine)\n",
    "file_utip = pd.read_excel('Lien_Dons_crowdfunding_tipers_time_JSON_utip.xlsx')\n",
    "df_utip = pd.DataFrame(file_utip)\n",
    "file_tipeee = pd.read_excel('Lien_Dons_crowdfunding_tipers_time_JSON_tipeee.xlsx')\n",
    "df_tipeee = pd.DataFrame(file_tipeee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = df_utip.merge(df, on = \"wa_id\",  how = \"left\") \n",
    "f3.to_excel(\"Results_utip.xlsx\", index = False) \n",
    "f4 = df_tipeee.merge(df, on = \"wa_id\",  how = \"left\") \n",
    "f4.to_excel(\"Results_tipeee.xlsx\", index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935fe0c2",
   "metadata": {},
   "source": [
    "### TOUTE (utip/tipeee et les autres platformes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcda2a5",
   "metadata": {},
   "source": [
    "On réalise aussi les mêmes étapes sur toutes les données des fichiers JSON. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df_crowd_lien_id[df_crowd_lien_id['Lien'] == 'argent\"}' ].index #certains liens ne sont pas conformes ou créé des erreurs, nous les avons donc enlevés\n",
    "df_crowd_lien_id.drop(indexNames , inplace=True)\n",
    "indexNames = df_crowd_lien_id[df_crowd_lien_id['Lien'] == 'https://www.patreon.com/posts/47954104' ].index\n",
    "df_crowd_lien_id.drop(indexNames , inplace=True)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cdd4ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "lien_crowd = list(df_crowd_lien_id.Lien)\n",
    "nom_crowd = list(df_crowd_lien_id.Crowdfunding)\n",
    "don_tipeee_utip = []\n",
    "don_tiper_utip = []\n",
    "lien_tipeee_utip = []\n",
    "time_final = []\n",
    "crowd_tipeee_utip = []\n",
    "id_yt_f = []\n",
    "\n",
    "for i in range(len(lien_crowd)):\n",
    "    url_f, found_don_f, found_tiper_f, time_f = function_get_sum_donation(lien_crowd[i])\n",
    "    don_tipeee_utip.append(found_don_f) \n",
    "    don_tiper_utip.append(found_tiper_f)\n",
    "    time_final.append(time_f)\n",
    "    lien_tipeee_utip.append(url_f)\n",
    "    crowd_tipeee_utip.append(nom_crowd[i])\n",
    "    id_yt_f.append(id_yt[i])\n",
    "    print(i)\n",
    "    print(lien_crowd[i])\n",
    "        \n",
    "df_crowd_lien_don_tipers_tipeee_utip=pd.DataFrame({'wa_id': id_yt_f, 'Crowdfunding': crowd_tipeee_utip, 'Lien': lien_tipeee_utip, 'Don': don_tipeee_utip, 'Nb Tipers': don_tiper_utip, 'Temps':time_final})\n",
    "df_crowd_lien_don_tipers_tipeee_utip.to_csv('Lien_Dons_crowdfunding_tipers_time_JSON_utip_tipeee_f.csv')\n",
    "df_crowd_lien_don_tipers_tipeee_utip.to_excel('Lien_Dons_crowdfunding_tipers_time_JSON_utip_tipeee_f.xlsx')\n",
    "print('end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ccbfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crowd_lien_don_tipers_tipeee_utip=pd.DataFrame({'wa_id': id_yt_f, 'Crowdfunding': crowd_tipeee_utip, 'Lien': lien_tipeee_utip, 'Don': don_tipeee_utip, 'Nb Tipers': don_tiper_utip, 'Temps':time_final})\n",
    "df_crowd_lien_don_tipers_tipeee_utip.to_csv('Lien_Dons_crowdfunding_tipers_time_JSON_utip_tipeee_f.csv')\n",
    "df_crowd_lien_don_tipers_tipeee_utip.to_excel('Lien_Dons_crowdfunding_tipers_time_JSON_utip_tipeee_f.xlsx')\n",
    "print(len(df_crowd_lien_don_tipers_tipeee_utip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6a0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
